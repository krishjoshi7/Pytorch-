{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPBL5ECgg8nXKRukk2ZWnQN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishjoshi7/Pytorch-/blob/main/Pytorch_fundamental_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accesing a  GPU"
      ],
      "metadata": {
        "id": "URqCtKoBcaB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPUs = faster computation on numbers,\n",
        "thanks to CUDA + NVIDIA hardware + Pytorch woking behind the sence to make everything hunky dory(good)"
      ],
      "metadata": {
        "id": "bz9H_rxrclnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting a GPU\n",
        "* Easisest  - Use Google Colab for a free Gpu (otions to upgrade as well)\n",
        "* Use your own GPU - takes a little bit of setup and requires the  investment of purchasing a GPU\n",
        "* Use cloud computing - GCP, AWS, Azure, these servies allow you to rent computer on the cloud and access them"
      ],
      "metadata": {
        "id": "66_h2yvmdGqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "for option 2 and 3 PyTorch + GPU drivers (CUDA) takes a little bit of setting up,\n",
        "* To do this refer to PyTorch setup doucmentation"
      ],
      "metadata": {
        "id": "C619vU-Mey8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check for GPU access with PyTorch"
      ],
      "metadata": {
        "id": "XRrAWvIlgG6G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWc14ySGb1Qo",
        "outputId": "7c7f0ac5-2271-4ce6-db20-0aed4ba56fea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#check for GPU access with PyTorch\n",
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup device agnostic code"
      ],
      "metadata": {
        "id": "XmPLqkByg0lS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device =\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PreEG0BFgzbN",
        "outputId": "c58e9f22-4043-4b87-8dbb-a80a36029e13"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " For Pytorch since it's capable of running computer on the GPU or CPU, it's best practice to setup device agnostic code :\n",
        "\n",
        " E.g run on GPU if avialable, else default to CPU"
      ],
      "metadata": {
        "id": "4FQTtQZghrAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of devices\n",
        "torch.cuda.device_count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzwLEbJbhHX-",
        "outputId": "0e887e20-d874-428d-87cd-b1e68f37a16a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting tensors (and models) on the GPU\n",
        "\n",
        "* The reason we want our tensor/model on the GPU is because using a GPU result in faster computation"
      ],
      "metadata": {
        "id": "07GjGbO4ifhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a tensor (default on the CPU)\n",
        "tensor = torch.tensor([1, 2, 3], device=\"cpu\")\n",
        "\n",
        "# Tensor not on GPU\n",
        "print(tensor, tensor.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRWn0EnahW_b",
        "outputId": "844eba0f-a4f5-452c-9f8a-b5bda68079ab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3]) cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move tensor to GPU (if available)\n",
        "tensor_on_gpu = tensor.to(device)\n",
        "tensor_on_gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXJ5nou4jmJn",
        "outputId": "58956422-340c-46b8-e108-5590a11bbac5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Numpy only works with cpu"
      ],
      "metadata": {
        "id": "s6CkSC6kk_iO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Moving tensor back to the CPU"
      ],
      "metadata": {
        "id": "FhHoRwg-rDOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If tensor is on GPU can't transform it to NumPy\n",
        "tensor_on_gpu.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "sPEOWLxakADJ",
        "outputId": "90e4158f-2873-412c-eb67-9155fe7dd74e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-a8b3da44f981>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# If tensor is on GPU can't transform it to NumPy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtensor_on_gpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To fix the GPU tensor with Numpy issue, we can frist set it to the CPU\n",
        "tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\n",
        "tensor_back_on_cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "az0l3XeEk735",
        "outputId": "043934d2-d096-4c8a-8b0d-699880041a0b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_on_gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjoI9JUQsTAZ",
        "outputId": "a768f83a-1ee4-42e3-dbcc-f958804cd9ad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}